Script started on 2022-12-06 18:26:42-05:00

**PRE-WORK** To find all the replies from both the original and nolf2 file, I used grep to find all "replied_to" tweets and awk to remove all
the bots from the file. I stored the results within downloaded_tweets_extend_original_nolf2_REPLIES.NOBOTS.tsv and downloaded_tweets_extend_original_nolf2_REPLIES.NOBOTS.tsv
respectively. Then, I used cat to combine the replies found in both files and stored it in replies_all.tsv. Then, using sort with the unique option,
on the tweet ID column, I removed all duplicates found in both files. Then, I found the authors who received three or more replies by using cut
on the sixth column containing author ids, and sort and uniq to filter for authors who recevied three or more replies from other users. Lastly,
I used a for loop to iterate through all the authors who had three or more replies and find all the tweets in which the replied_to (column 6) field contained
the author using awk. I performed the check on the replies_all_non_duplicates.tsv, containing the combined replies from both the original and nolf files.
I stored the result final_cleaned_three_or_more.tsv, which contains only the cluster of replies for authors that received three or more replies.
[sohan@sjsu twitter]$ grep type=replied_to downloaded_tweets_extend_original_nolf2.tsv |  awk -F'\t' '$2!=$6{print $0}' > downloaded_tweets_extend_original_nolf2_REPLIES.NOBOTS.tsv
[sohan@sjsu twitter]$ grep type=replied_to  downloaded_tweets_extend_nolf2.tsv |  awk -F'\t' '$2!=$6{print $0}' > downloaded_tweets_extend_nolf2_REPLIES.NOBOTS.tsv
[sohan@sjsu twitter]$ cat downloaded_tweets_extend_original_nolf2_REPLIES.NOBOTS.tsv downloaded_tweets_extend_nolf2_REPLIES.NOBOTS.tsv > replies_all.tsv
[sohan@sjsu twitter]$ sort -uk1,1 replies_all.tsv > replies_all_non_duplicates.tsv
[sohan@sjsu twitter]$ cut -f6 replies_all_non_duplicates.tsv | sort -n | uniq -c  | awk '$1>=3{print $2}' > authors_three_or_more_replies.tsv
[sohan@sjsu twitter]$ for i in `cat authors_three_or_more_replies.tsv`; do awk -F'\t' -v word=$i 'word==$6{print $0}' replies_all_non_duplicates.tsv; done > final_cleaned_three_or_more.tsv

1. There are a total of 445 influential users in the cluster, which was found by using cut on the sixth column (replied_to) containing all author ids
and sort, uniq, and wc to find total number of unique authors.
[sohan@sjsu twitter]$ cut -f6 final_cleaned_three_or_more.tsv | sort -n | uniq | wc | awk '{print $1}' 
445

2. There are a total of 668 users who replied to the given cluster of influential users, which was found by using cut on the second column (author_id)
and sort, uniq, and wc to find total number of unique users.
[sohan@sjsu twitter]$ cut -f2 final_cleaned_three_or_more.tsv | sort -n | uniq | wc | awk '{print $1}' 
680

3. There are a total of 28 users who made a reply and received a reply themselves. To find the users who made a reply
and received a reply themselves, I made two files containing all unique repliers in the cluster using
cut on the second column and authors using cut on the sixth column of the combined files respectively.
Then, I used join to find the common users ids between the two files, to find those who made a reply
and received a reply, which was a total of 28.
[sohan@sjsu twitter]$ cut -f2 final_cleaned_three_or_more.tsv | sort | uniq > reply_users.tsv
[sohan@sjsu twitter]$ cut -f6 final_cleaned_three_or_more.tsv | sort | uniq > original_authors.tsv
[sohan@sjsu twitter]$ join -1 1 -2 1 reply_users.tsv original_authors.tsv > common_repliers_authors.tsv
[sohan@sjsu twitter]$ wc common_repliers_authors.tsv | awk '{print $1}' 
28

4. To find the dates in which most replies were received, I used cut in the third column of the file to extract the datestamps
and sort and uniq to find the count of replies on each given date. Then, I found the top 10 dates, using sort -nr on the count
field and head to find the top 10 dates.
[sohan@sjsu twitter]$ cut -f3 final_cleaned_three_or_more.tsv | awk '{print $1}' | sort | uniq -c | sort -nr -k 1 | head
     37 2022-02-24
     35 2022-02-23
     34 2022-02-12
     33 2022-02-17
     31 2022-02-26
     30 2022-04-29
     30 2022-02-16
     29 2022-02-14
     28 2022-04-25
     28 2022-04-08

5. To find the most common words with the subset of three or more replies cluster, I used cut on the eight column containing the
text and gsub to filter out punctuation and any other erroneous stop words. Then I used a for loop to find word counts for each word
found. Then, I used fgrep on the output with the stopwords file (/usr/share/groff/current/eign) to remove all stop words. Lastly, I
sorted the results in reverse to find the highest word counts and head to display the top 10 words.
[sohan@sjsu twitter]$ cut -f8 final_cleaned_three_or_more.tsv | awk '{gsub(","," ",$0); gsub("\\."," ",$0); gsub("#"," ",$0); gsub("<[^>]*>"," ",$0); gsub("\\\\"," ",$0); gsub(" [a-z] "," ",$0); gsub("-"," ",$0); gsub("https://t"," ",$0); gsub("has"," ",$0); gsub("&amp;"," ",$0); gsub(/"/," ",$0); sub(" 2 "," ",$0); print tolower($0)}' | awk '{for(i=1; i <= NF; i++) wordCounts[$i]++} END { for (i in wordCounts) print i","wordCounts[i]}' | fgrep -v -w -f /usr/share/groff/current/eign |  sort -nr -t ',' -k2 | head -11
ford,153
covid19,106
putinhitler,102
need,98
doug,94
ontario,92
covid,92
canada,83
standwithukraine,77
bcpoli,69
conservative,68
[sohan@sjsu twitter]$ exit

Script done on 2022-12-06 18:30:29-05:00
